{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylEStXuquTVc",
        "outputId": "55d4c10a-0719-490b-c728-f9409b73f07d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jS7PCY6uA2a_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
        "df = pd.read_csv('https://files.grouplens.org/datasets/movielens/ml-100k/u.data', sep='\\t', names=column_names)\n",
        "df = df.drop('timestamp', axis=1)\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da_yKACjA43K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Remap user_id and item_id to a zero-based index\n",
        "user_mapping = {user_id: idx for idx, user_id in enumerate(train_df['user_id'].unique())}\n",
        "item_mapping = {item_id: idx for idx, item_id in enumerate(train_df['item_id'].unique())}\n",
        "\n",
        "train_df['user_id'] = train_df['user_id'].map(user_mapping)\n",
        "train_df['item_id'] = train_df['item_id'].map(item_mapping)\n",
        "test_df['user_id'] = test_df['user_id'].map(user_mapping)\n",
        "test_df['item_id'] = test_df['item_id'].map(item_mapping)\n",
        "\n",
        "# Remove any rows in test_df with unmapped (NaN) user or item IDs\n",
        "test_df = test_df.dropna().astype({'user_id': 'int', 'item_id': 'int'})\n",
        "\n",
        "class MatrixFactorization(nn.Module):\n",
        "    def __init__(self, num_users, num_items, latent_dim=10):\n",
        "        super(MatrixFactorization, self).__init__()\n",
        "        self.user_factors = nn.Embedding(num_users, latent_dim)\n",
        "        self.item_factors = nn.Embedding(num_items, latent_dim)\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        return (self.user_factors(user) * self.item_factors(item)).sum(1)\n",
        "\n",
        "def matrix_factorization(train_data, test_data, output_dir):\n",
        "    num_users = train_data['user_id'].nunique()\n",
        "    num_items = train_data['item_id'].nunique()\n",
        "    model = MatrixFactorization(num_users, num_items)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(20):  # Adjust epochs as needed\n",
        "        for _, row in train_data.iterrows():\n",
        "            user = torch.LongTensor([row['user_id']])\n",
        "            item = torch.LongTensor([row['item_id']])\n",
        "            rating = torch.FloatTensor([row['rating']])\n",
        "\n",
        "            prediction = model(user, item)\n",
        "            loss = criterion(prediction, rating)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Prediction\n",
        "    predictions = []\n",
        "    for _, row in test_data.iterrows():\n",
        "        user = torch.LongTensor([row['user_id']])\n",
        "        item = torch.LongTensor([row['item_id']])\n",
        "        predicted_rating = model(user, item).item()\n",
        "        original_user_id = list(user_mapping.keys())[list(user_mapping.values()).index(row['user_id'])]\n",
        "        original_item_id = list(item_mapping.keys())[list(item_mapping.values()).index(row['item_id'])]\n",
        "        predictions.append((original_user_id, original_item_id, predicted_rating))\n",
        "\n",
        "    # Save predictions to Google Drive\n",
        "    pd.DataFrame(predictions, columns=['user_id', 'item_id', 'predicted_rating']).to_csv(\n",
        "        output_dir + 'matrix_factorization_predictions.csv', index=False\n",
        "    )\n",
        "\n",
        "# Define output directory and run the function\n",
        "output_dir = '/content/drive/MyDrive/submission/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "matrix_factorization(train_df, test_df, output_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}