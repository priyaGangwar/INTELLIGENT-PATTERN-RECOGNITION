{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yueaMFdEURx",
        "outputId": "9e000861-f28c-407c-9c9f-afac8d4b0068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
        "df = pd.read_csv('https://files.grouplens.org/datasets/movielens/ml-100k/u.data', sep='\\t', names=column_names)\n",
        "df = df.drop('timestamp', axis=1)\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "90k6bVcgEbjx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # Ensure this is at the very top\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import pandas as pd\n",
        "\n",
        "# Define the Autoencoder model\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, num_items):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(num_items, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_items),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "def autoencoder_model(train_data, test_data, output_dir):\n",
        "    num_items = train_data['item_id'].nunique()\n",
        "    user_item_matrix = train_data.pivot(index='user_id', columns='item_id', values='rating').fillna(0).values\n",
        "    model = Autoencoder(num_items)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(50):  # Adjust epochs as needed\n",
        "        inputs = torch.FloatTensor(user_item_matrix)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, inputs)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Prediction\n",
        "    predictions = []\n",
        "    for _, row in test_data.iterrows():\n",
        "        user, item = row['user_id'], row['item_id']\n",
        "        if user - 1 < len(user_item_matrix) and item - 1 < num_items:\n",
        "            user_ratings = model(torch.FloatTensor(user_item_matrix[user - 1:user])).detach().numpy()\n",
        "            predictions.append((user, item, user_ratings[0][item - 1]))\n",
        "        else:\n",
        "            # Handle cases where user or item is out of bounds\n",
        "            predictions.append((user, item, None))  # or some default value\n",
        "\n",
        "    # Save predictions to Google Drive\n",
        "    pd.DataFrame(predictions, columns=['user_id', 'item_id', 'predicted_rating']).to_csv(\n",
        "        output_dir + 'autoencoder_predictions.csv', index=False\n",
        "    )\n",
        "\n",
        "# Define output directory and run the function\n",
        "output_dir = '/content/drive/MyDrive/submission/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "autoencoder_model(train_df, test_df, output_dir)\n"
      ],
      "metadata": {
        "id": "CQyxPGNNErbq"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}